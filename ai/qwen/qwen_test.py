from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
from pathlib import Path
import torch
import time

total_time = 0 # ì‹œê°„ ê³„ì‚°ìš©
base_dir = Path(__file__).resolve().parent.parent # í˜„ì¬ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì • : KSEB/ai

# ====== ì„¤ì • ======
# 1ï¸âƒ£ ì´ë¯¸ì§€ ê²½ë¡œ (ì‚¬ìš© ì•ˆ í•  ë• None)
# image_path = base_dir / "data" / "img" / "img_001.jpg"
image_path = None

# 2ï¸âƒ£ ìºì‹œëœ ì„ë² ë”© ê²½ë¡œ (ì‚¬ìš© ì•ˆ í•  ë• None)
embedding_path = base_dir / "qwen" / "cached_embeds" / "img_001.pt"

prompt_text = "ì´ ë¬¸ì„œë¥¼ ë…¸ì¸ì„ ìœ„í•´ ì‰½ê²Œ ì„¤ëª…í•´ì¤˜"


# ëª¨ë¸ ë¡œë“œ
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-VL-7B-Instruct", 
    torch_dtype=torch.float16, # rtx 4060 ì—ì„œ ë©”ëª¨ë¦¬/ì†ë„ì— íš¨ìœ¨ì 
    device_map="auto"
)
model.eval()  # í‰ê°€ ëª¨ë“œ ì „í™˜ : ì¶”ë¡ ì‹œì— ì¼ê´€ì„± ìœ ì§€í•˜ê¸° ìœ„í•¨
print(f"ëª¨ë¸ ë””ë°”ì´ìŠ¤: {model.device if hasattr(model, 'device') else 'ë©€í‹° ë””ë°”ì´ìŠ¤'}")

# ì¡°ê±´ì— ë”°ë¼ processor ì´ˆê¸°í™”
if image_path and image_path.exists():
    # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì ˆì´ í•„ìš”í•œ ê²½ìš°
    min_pixels = 256 * 14 * 14
    max_pixels = 1280 * 14 * 14
    processor = AutoProcessor.from_pretrained(
        "Qwen/Qwen2.5-VL-7B-Instruct",
        min_pixels=min_pixels,
        max_pixels=max_pixels
    )
else:
    # ì„ë² ë”© ì‚¬ìš©í•  ê²½ìš° í¬ê¸° ì¡°ì ˆ í•„ìš” ì—†ìŒ
    processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct")

# ====== ë©”ì‹œì§€ ì¤€ë¹„ ======
if image_path and image_path.exists():
    image = Image.open(image_path).convert("RGB")
    messages = [
        {"role": "user", "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": prompt_text}
        ]}
    ]
    image_inputs, video_inputs = process_vision_info(messages)
elif embedding_path and embedding_path.exists():
    # ìºì‹œëœ ì„ë² ë”© ë¶ˆëŸ¬ì˜¤ê¸°
    image_inputs = torch.load(embedding_path, weights_only=False)
    video_inputs = None
    messages = [
        {"role": "user", "content": [
            {"type": "image", "image": "<cached>"},
            {"type": "text", "text": prompt_text}
        ]}
    ]
else:
    raise FileNotFoundError("ì´ë¯¸ì§€ë‚˜ ì„ë² ë”© íŒŒì¼ ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤.")

# ====== ì…ë ¥ ìƒì„± ======
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)
inputs = inputs.to(model.device)

# ====== ì¶”ë¡  ======
start = time.time()
with torch.no_grad(): # no_grad : ì¶”ë¡ ì‹œì—ë§Œ ì‚¬ìš©. gradientë¥¼ ê³„ì‚°í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ëœ»
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=256, # í˜¹ì€ 128
        do_sample=False # ìƒ˜í”Œë§ì„ í•˜ì§€ ì•Šê³ , greedy ë°©ì‹ìœ¼ë¡œ ì¶œë ¥ ìƒì„±
    )
    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )
end = time.time()

# ====== ê²°ê³¼ ì¶œë ¥ ======
print(f"ğŸ•’ ì²˜ë¦¬ ì‹œê°„: {end - start:.2f}ì´ˆ")
print("ğŸ§  ëª¨ë¸ ì‘ë‹µ:\n", output_text[0])