from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
from pathlib import Path
import torch
import time

total_time = 0 # ì‹œê°„ ê³„ì‚°ìš©

# ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
base_dir = Path(__file__).resolve().parent.parent # í˜„ì¬ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì • : KSEB/ai
image_path = base_dir / "data" / "img" / "img_001.jpg"
image = Image.open(image_path).convert("RGB")

# ëª¨ë¸ ë¡œë“œ
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-VL-7B-Instruct", 
    torch_dtype=torch.float16, # rtx 4060 ì—ì„œ ë©”ëª¨ë¦¬/ì†ë„ì— íš¨ìœ¨ì 
    device_map="auto"
)

model.eval()  # í‰ê°€ ëª¨ë“œ ì „í™˜ : ì¶”ë¡ ì‹œì— ì¼ê´€ì„± ìœ ì§€í•˜ê¸° ìœ„í•¨

print(f"ëª¨ë¸ ë””ë°”ì´ìŠ¤: {model.device if hasattr(model, 'device') else 'ë©€í‹° ë””ë°”ì´ìŠ¤'}")

min_pixels = 256 * 14 * 14
max_pixels = 1280 * 14 * 14

processor = AutoProcessor.from_pretrained(
    "Qwen/Qwen2.5-VL-7B-Instruct",
    use_fast=True,
    min_pixels=min_pixels,
    max_pixels=max_pixels
    )

# ğŸ” ë©”ì‹œì§€ êµ¬ì¡° (ë¡œì»¬ ì´ë¯¸ì§€ ì‚¬ìš©)
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": "ì´ ë¬¸ì„œë¥¼ ë…¸ì¸ì„ ìœ„í•´ ì‰½ê²Œ ì„¤ëª…í•´ì¤˜"},
        ],
    }
]


# í…ìŠ¤íŠ¸ prompt ìƒì„±
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

# ì‹œê° ì •ë³´ ì²˜ë¦¬
image_inputs, video_inputs = process_vision_info(messages)

# ìµœì¢… ëª¨ë¸ ì…ë ¥ ìƒì„±
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)
inputs = inputs.to(model.device)

print(f"ì…ë ¥ ë””ë°”ì´ìŠ¤: {inputs.input_ids.device}")

start = time.time()
# ì¶œë ¥ ìƒì„±
with torch.no_grad(): # no_grad : ì¶”ë¡ ì‹œì—ë§Œ ì‚¬ìš©. gradientë¥¼ ê³„ì‚°í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ëœ»
    generated_ids = model.generate(**inputs, 
                                   max_new_tokens=256, # 128
                                   do_sample=False, # ìƒ˜í”Œë§ì„ í•˜ì§€ ì•Šê³ , greedy ë°©ì‹ìœ¼ë¡œ ì¶œë ¥ ìƒì„±
                                   )
    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )
end = time.time()
elapsed = end - start
print(f"ì²˜ë¦¬ ì‹œê°„: {elapsed:.2f}ì´ˆ")
print("ğŸ§  ëª¨ë¸ ì‘ë‹µ:\n", output_text[0])
